#!/usr/bin/env python3
"""
URL ê²€ì¦ ê²°ê³¼ ìš”ì•½ ìŠ¤í¬ë¦½íŠ¸
ì—¬ëŸ¬ ë°ì´í„°ì…‹ì˜ URL ê²€ì¦ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìš”ì•½ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import os
import json
import pandas as pd
from typing import Dict, List
import argparse
from datetime import datetime


def load_validation_results(dataset_name: str) -> Dict:
    """ê²€ì¦ ê²°ê³¼ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    result_file = f"url_validation_results_{dataset_name}.json"
    if os.path.exists(result_file):
        with open(result_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return None


def analyze_dataset(dataset_name: str) -> Dict:
    """ë°ì´í„°ì…‹ì˜ URL ê²€ì¦ ê²°ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    results = load_validation_results(dataset_name)
    if not results:
        return None
    
    # ê¸°ë³¸ í†µê³„
    stats = {
        'dataset': dataset_name,
        'total_asins': results['total_asins'],
        'asins_with_url': results['asins_with_url'],
        'asins_without_url': results['asins_without_url'],
        'url_coverage_rate': results['url_coverage_rate'],
        'validated_urls': results['validated_urls'],
        'valid_urls': results['valid_urls'],
        'invalid_urls': results['invalid_urls'],
        'validation_success_rate': results['validation_success_rate'],
        'validation_time': results['validation_time']
    }
    
    # ì˜¤ë¥˜ ìœ í˜•ë³„ ë¶„ì„
    error_types = {}
    if 'detailed_results' in results:
        for asin, data in results['detailed_results'].items():
            if not data['is_valid']:
                error_msg = data['message']
                if 'HTTP 404' in error_msg:
                    error_types['404 Not Found'] = error_types.get('404 Not Found', 0) + 1
                elif 'Timeout' in error_msg:
                    error_types['Timeout'] = error_types.get('Timeout', 0) + 1
                elif 'Connection Error' in error_msg:
                    error_types['Connection Error'] = error_types.get('Connection Error', 0) + 1
                elif 'Not image' in error_msg:
                    error_types['Not Image'] = error_types.get('Not Image', 0) + 1
                else:
                    error_types['Other'] = error_types.get('Other', 0) + 1
    
    stats['error_breakdown'] = error_types
    return stats


def generate_summary_report(datasets: List[str], output_file: str = None):
    """ì—¬ëŸ¬ ë°ì´í„°ì…‹ì˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìš”ì•½ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    all_stats = []
    
    for dataset in datasets:
        stats = analyze_dataset(dataset)
        if stats:
            all_stats.append(stats)
    
    if not all_stats:
        print("ë¶„ì„í•  ìˆ˜ ìˆëŠ” ê²€ì¦ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # DataFrameìœ¼ë¡œ ë³€í™˜
    df = pd.DataFrame(all_stats)
    
    # ë³´ê³ ì„œ ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ìƒì„±
    report_lines = []
    
    report_lines.append("=" * 80)
    report_lines.append("URL ê²€ì¦ ê²°ê³¼ ì¢…í•© ë³´ê³ ì„œ")
    report_lines.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("=" * 80)
    report_lines.append("")
    
    # ê¸°ë³¸ í†µê³„ í…Œì´ë¸”
    report_lines.append("ğŸ“Š ê¸°ë³¸ í†µê³„")
    report_lines.append("-" * 80)
    report_lines.append(f"{'ë°ì´í„°ì…‹':<12} {'ì „ì²´ ASIN':<10} {'URL ë³´ìœ ':<10} {'URL ì—†ìŒ':<10} {'ë³´ìœ ìœ¨(%)':<10} {'ê²€ì¦ ì„±ê³µë¥ (%)':<15}")
    report_lines.append("-" * 80)
    
    for _, row in df.iterrows():
        report_lines.append(f"{row['dataset']:<12} {row['total_asins']:<10} {row['asins_with_url']:<10} {row['asins_without_url']:<10} "
                           f"{row['url_coverage_rate']:<10.2f} {row['validation_success_rate']:<15.2f}")
    
    report_lines.append("")
    
    # ì „ì²´ ìš”ì•½
    total_asins = df['total_asins'].sum()
    total_with_url = df['asins_with_url'].sum()
    total_without_url = df['asins_without_url'].sum()
    total_validated = df['validated_urls'].sum()
    total_valid = df['valid_urls'].sum()
    total_invalid = df['invalid_urls'].sum()
    
    report_lines.append("ğŸ“ˆ ì „ì²´ ìš”ì•½")
    report_lines.append("-" * 80)
    report_lines.append(f"ì „ì²´ ASIN ê°œìˆ˜: {total_asins:,}")
    report_lines.append(f"ì´ë¯¸ì§€ URLì´ ìˆëŠ” ASIN: {total_with_url:,} ({total_with_url/total_asins*100:.2f}%)")
    report_lines.append(f"ì´ë¯¸ì§€ URLì´ ì—†ëŠ” ASIN: {total_without_url:,} ({total_without_url/total_asins*100:.2f}%)")
    report_lines.append(f"ê²€ì¦ëœ URL ê°œìˆ˜: {total_validated:,}")
    report_lines.append(f"ì ‘ì† ê°€ëŠ¥í•œ URL: {total_valid:,} ({total_valid/total_validated*100:.2f}%)")
    report_lines.append(f"ì ‘ì† ë¶ˆê°€ëŠ¥í•œ URL: {total_invalid:,} ({total_invalid/total_validated*100:.2f}%)")
    report_lines.append("")
    
    # ì˜¤ë¥˜ ìœ í˜•ë³„ ë¶„ì„
    report_lines.append("ğŸš¨ ì˜¤ë¥˜ ìœ í˜•ë³„ ë¶„ì„")
    report_lines.append("-" * 80)
    
    all_error_types = {}
    for _, row in df.iterrows():
        for error_type, count in row['error_breakdown'].items():
            all_error_types[error_type] = all_error_types.get(error_type, 0) + count
    
    if all_error_types:
        sorted_errors = sorted(all_error_types.items(), key=lambda x: x[1], reverse=True)
        report_lines.append(f"{'ì˜¤ë¥˜ ìœ í˜•':<20} {'ê°œìˆ˜':<10} {'ë¹„ìœ¨(%)':<10}")
        report_lines.append("-" * 40)
        for error_type, count in sorted_errors:
            percentage = count / total_invalid * 100 if total_invalid > 0 else 0
            report_lines.append(f"{error_type:<20} {count:<10} {percentage:<10.2f}")
    else:
        report_lines.append("ì ‘ì† ë¶ˆê°€ëŠ¥í•œ URLì´ ì—†ìŠµë‹ˆë‹¤.")
    
    report_lines.append("")
    
    # ë°ì´í„°ì…‹ë³„ ìƒì„¸ ì •ë³´
    report_lines.append("ğŸ“‹ ë°ì´í„°ì…‹ë³„ ìƒì„¸ ì •ë³´")
    report_lines.append("-" * 80)
    
    for _, row in df.iterrows():
        report_lines.append(f"\nğŸ” {row['dataset'].upper()} ë°ì´í„°ì…‹:")
        report_lines.append(f"  â€¢ ì „ì²´ ASIN: {row['total_asins']:,}ê°œ")
        report_lines.append(f"  â€¢ URL ë³´ìœ  ASIN: {row['asins_with_url']:,}ê°œ ({row['url_coverage_rate']:.2f}%)")
        report_lines.append(f"  â€¢ URL ì—†ìŒ ASIN: {row['asins_without_url']:,}ê°œ")
        report_lines.append(f"  â€¢ ê²€ì¦ëœ URL: {row['validated_urls']:,}ê°œ")
        report_lines.append(f"  â€¢ ê²€ì¦ í†µê³¼í•œ URL: {row['valid_urls']:,}ê°œ ({row['validation_success_rate']:.2f}%)")
        report_lines.append(f"  â€¢ ê²€ì¦ ì‹¤íŒ¨í•œ URL: {row['invalid_urls']:,}ê°œ")
        report_lines.append(f"  â€¢ ê²€ì¦ ì†Œìš” ì‹œê°„: {row['validation_time']:.2f}ì´ˆ")
        
        if row['error_breakdown']:
            report_lines.append("  â€¢ ì˜¤ë¥˜ ìœ í˜•:")
            for error_type, count in row['error_breakdown'].items():
                report_lines.append(f"    - {error_type}: {count}ê°œ")
        else:
            report_lines.append("  â€¢ ëª¨ë“  URLì´ ì •ìƒì ìœ¼ë¡œ ì ‘ì† ê°€ëŠ¥")
    
    report_lines.append("")
    report_lines.append("=" * 80)
    report_lines.append("ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
    report_lines.append("=" * 80)
    
    # ë³´ê³ ì„œ ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ê²°í•©
    report_content = "\n".join(report_lines)
    
    # ì½˜ì†”ì— ì¶œë ¥
    print(report_content)
    
    # íŒŒì¼ë¡œ ì €ì¥
    if output_file is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"url_validation_report_{timestamp}.txt"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"\nğŸ“„ ë³´ê³ ì„œê°€ '{output_file}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def main():
    parser = argparse.ArgumentParser(description='URL ê²€ì¦ ê²°ê³¼ ìš”ì•½ ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument('--datasets', nargs='+', default=['beauty', 'clothing', 'home', 'sports', 'toys'], 
                       help='ë¶„ì„í•  ë°ì´í„°ì…‹ ëª©ë¡')
    parser.add_argument('--output', '-o', type=str, default=None,
                       help='ì¶œë ¥ íŒŒì¼ëª… (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ íŒŒì¼ëª… ìë™ ìƒì„±)')
    
    args = parser.parse_args()
    
    # ì‹¤ì œë¡œ ê²€ì¦ ê²°ê³¼ íŒŒì¼ì´ ìˆëŠ” ë°ì´í„°ì…‹ë§Œ í•„í„°ë§
    available_datasets = []
    for dataset in args.datasets:
        result_file = f"url_validation_results_{dataset}.json"
        if os.path.exists(result_file):
            available_datasets.append(dataset)
        else:
            print(f"âš ï¸  {dataset} ë°ì´í„°ì…‹ì˜ ê²€ì¦ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    if available_datasets:
        generate_summary_report(available_datasets, args.output)
    else:
        print("ë¶„ì„í•  ìˆ˜ ìˆëŠ” ê²€ì¦ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == '__main__':
    main()
