{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb2221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 (Text):  0.0464\n",
      "Recall@5 (Image): 0.0394\n",
      "Recall@5 (MM):    0.0500\n",
      "\n",
      "================================================================================\n",
      "Recall@5 Analysis - Embedding Overlap\n",
      "================================================================================\n",
      "\n",
      "[1] Exclusive Hits - 오직 해당 임베딩만 맞춘 샘플\n",
      "  Text only:    160 samples (0.72%)\n",
      "  Image only:   182 samples (0.81%)\n",
      "  MM only:      106 samples (0.47%)\n",
      "\n",
      "[2] Pairwise Hits - 정확히 두 개만 맞춘 샘플\n",
      "  Text + Image (MM X):    13 samples (0.06%)\n",
      "  Text + MM (Image X):   327 samples (1.46%)\n",
      "  Image + MM (Text X):   149 samples (0.67%)\n",
      "\n",
      "[3] Common & Missing\n",
      "  All three hit:   537 samples (2.40%)\n",
      "  All miss:      20890 samples (93.41%)\n",
      "\n",
      "[4] Verification\n",
      "  Sum: 160 + 182 + 106 + 13 + 327 + 149 + 537 + 20890 = 22364\n",
      "  Total samples: 22364\n",
      "  Match: True\n",
      "\n",
      "[5] Text vs Image (기존 통계)\n",
      "  Text fail & Image hit:   331\n",
      "  Text hit & Image fail:   487\n",
      "  Both hit:                550\n",
      "  Both fail:             20996\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os, json, pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ===========================\n",
    "# 1. 데이터 로드\n",
    "# ===========================\n",
    "config = {}\n",
    "config[\"dataset\"] = \"beauty\"\n",
    "config[\"text_encoder\"]  = \"qwen3vl_emb2b/text\"\n",
    "config[\"image_encoder\"] = \"qwen3vl_emb2b/image\"\n",
    "config[\"mm_encoder\"]    = \"qwen3vl_emb2b/text_image\"\n",
    "\n",
    "dataset_path = f'./dataset/{config[\"dataset\"]}'\n",
    "id_map = json.load(open(f'{dataset_path}/id_map.json', \"r\"))[\"item2id\"]\n",
    "\n",
    "text_feat_raw  = pickle.load(open(f'{dataset_path}/{config[\"text_encoder\"]}.pkl', \"rb\"))\n",
    "image_feat_raw = pickle.load(open(f'{dataset_path}/{config[\"image_encoder\"]}.pkl', \"rb\"))\n",
    "mm_feat_raw    = pickle.load(open(f'{dataset_path}/{config[\"mm_encoder\"]}.pkl', \"rb\"))\n",
    "\n",
    "test_data = torch.load('./saved_data/test_data.pth', weights_only=False)\n",
    "\n",
    "# ===========================\n",
    "# 2. Target 및 Query 준비\n",
    "# ===========================\n",
    "target = test_data.dataset.inter_feat['item_id']\n",
    "target_str = [str(t.item()) for t in target]\n",
    "user_sequence = test_data.dataset.inter_feat['item_id_list']\n",
    "seq_lens = test_data.dataset.inter_feat['item_length']\n",
    "last_item_indices = seq_lens - 1\n",
    "last_items = [user_sequence[i][last_item_indices[i]].item() for i in range(len(user_sequence))]\n",
    "last_items_str = [str(user_sequence[i][last_item_indices[i]].item()) for i in range(len(user_sequence))]\n",
    "\n",
    "last_items.append(7849)\n",
    "target = torch.cat([target, torch.tensor([7850])], dim=0)\n",
    "\n",
    "# ===========================\n",
    "# 3. Feature Mapping\n",
    "# ===========================\n",
    "item_num = test_data.dataset.field2id_token['item_id'].__len__()\n",
    "text_mapped_feat = np.zeros((item_num, text_feat_raw.shape[1]))\n",
    "image_mapped_feat = np.zeros((item_num, image_feat_raw.shape[1]))\n",
    "mm_mapped_feat = np.zeros((item_num, mm_feat_raw.shape[1]))\n",
    "\n",
    "for i, token in enumerate(test_data.dataset.field2id_token['item_id']):\n",
    "    if token == '[PAD]':\n",
    "        continue\n",
    "    token_idx = int(id_map[token])-1\n",
    "    text_mapped_feat[i] = text_feat_raw[token_idx]\n",
    "    image_mapped_feat[i] = image_feat_raw[token_idx]\n",
    "    mm_mapped_feat[i] = mm_feat_raw[token_idx]\n",
    "\n",
    "# ===========================\n",
    "# 4. Recommendation Functions\n",
    "# ===========================\n",
    "def topk_recommend(query_item_ids, all_item_emb, k=5, exclude_history=False, history_sets=None, exclude_self=True):\n",
    "    \"\"\"\n",
    "    query_item_ids: 각 유저의 마지막 아이템 ID 리스트 (len=N)\n",
    "    all_item_emb: (item_num, emb_dim) 전체 아이템 임베딩\n",
    "    k: 추천할 개수\n",
    "    exclude_self: True면 query 자기 자신은 추천에서 제외\n",
    "    \"\"\"\n",
    "    query_item_ids = np.array(query_item_ids)\n",
    "    N = len(query_item_ids)\n",
    "    \n",
    "    # query 임베딩\n",
    "    query_emb = all_item_emb[query_item_ids]  # (N, emb_dim)\n",
    "    \n",
    "    # 유사도 계산 (cosine similarity)\n",
    "    query_norm = np.linalg.norm(query_emb, axis=1, keepdims=True) + 1e-12\n",
    "    item_norm = np.linalg.norm(all_item_emb, axis=1, keepdims=True) + 1e-12\n",
    "    \n",
    "    query_emb_normed = query_emb / query_norm\n",
    "    item_emb_normed = all_item_emb / item_norm\n",
    "    \n",
    "    scores = query_emb_normed @ item_emb_normed.T  # (N, item_num)\n",
    "    \n",
    "    # exclude_self - 벡터화로 최적화\n",
    "    if exclude_self:\n",
    "        scores[np.arange(N), query_item_ids] = -np.inf\n",
    "    \n",
    "    # exclude_history\n",
    "    if exclude_history and history_sets is not None:\n",
    "        for i in range(N):\n",
    "            if len(history_sets[i]) > 0:\n",
    "                scores[i, list(history_sets[i])] = -np.inf\n",
    "    \n",
    "    # top-k - np.argpartition 사용으로 최적화 (전체 정렬 대신 top-k만 찾기)\n",
    "    # k+1개를 찾아서 exclude_self로 인한 자기 자신을 제외할 수 있도록 함\n",
    "    topk_indices = np.argpartition(-scores, k, axis=1)[:, :k]\n",
    "    # 각 행의 top-k를 정렬\n",
    "    topk_scores = np.take_along_axis(scores, topk_indices, axis=1)\n",
    "    sorted_idx = np.argsort(-topk_scores, axis=1)\n",
    "    topk_indices = np.take_along_axis(topk_indices, sorted_idx, axis=1)\n",
    "    topk_scores = np.take_along_axis(topk_scores, sorted_idx, axis=1)\n",
    "    \n",
    "    return topk_indices, topk_scores\n",
    "\n",
    "\n",
    "def recall_at_k(topk_ids, target):\n",
    "    \"\"\"\n",
    "    topk_ids: (N, k) 추천 결과\n",
    "    target: (N,) 정답 아이템\n",
    "    \n",
    "    Returns:\n",
    "    - recall: Recall@k 값\n",
    "    - hit: (N,) boolean 배열, hit[i]=True면 i번째 샘플이 top-k에 정답 포함\n",
    "    \"\"\"\n",
    "    target_np = target.cpu().numpy() if isinstance(target, torch.Tensor) else np.array(target)\n",
    "    N = topk_ids.shape[0]\n",
    "    \n",
    "    # 벡터화된 방식으로 hit 계산 (훨씬 빠름)\n",
    "    # 각 target이 해당 행의 topk_ids에 있는지 확인\n",
    "    hit = (topk_ids == target_np[:, np.newaxis]).any(axis=1)\n",
    "    \n",
    "    recall = np.mean(hit)\n",
    "    return recall, hit\n",
    "\n",
    "# ===========================\n",
    "# 5. Top-K 추천 수행\n",
    "# ===========================\n",
    "k = 5\n",
    "query_ids = last_items\n",
    "\n",
    "# Text 추천\n",
    "text_topk_ids, text_topk_scores = topk_recommend(\n",
    "    query_item_ids=query_ids,\n",
    "    all_item_emb=text_mapped_feat,\n",
    "    k=k,\n",
    "    exclude_history=False,\n",
    "    history_sets=None,\n",
    "    exclude_self=True,\n",
    ")\n",
    "\n",
    "# Image 추천\n",
    "img_topk_ids, img_topk_scores = topk_recommend(\n",
    "    query_item_ids=query_ids,\n",
    "    all_item_emb=image_mapped_feat,\n",
    "    k=k,\n",
    "    exclude_history=False,\n",
    "    history_sets=None,\n",
    "    exclude_self=True,\n",
    ")\n",
    "\n",
    "# MM 추천\n",
    "mm_topk_ids, mm_topk_scores = topk_recommend(\n",
    "    query_item_ids=query_ids,\n",
    "    all_item_emb=mm_mapped_feat,\n",
    "    k=k,\n",
    "    exclude_history=False,\n",
    "    history_sets=None,\n",
    "    exclude_self=True,\n",
    ")\n",
    "\n",
    "# Recall 계산\n",
    "text_recall, text_hit = recall_at_k(text_topk_ids, target)\n",
    "img_recall, img_hit = recall_at_k(img_topk_ids, target)\n",
    "mm_recall, mm_hit = recall_at_k(mm_topk_ids, target)\n",
    "\n",
    "print(f\"Recall@{k} (Text):  {text_recall:.4f}\")\n",
    "print(f\"Recall@{k} (Image): {img_recall:.4f}\")\n",
    "print(f\"Recall@{k} (MM):    {mm_recall:.4f}\")\n",
    "\n",
    "# ===========================\n",
    "# 6. 샘플 수 계산\n",
    "# ===========================\n",
    "# 각 임베딩이 독점적으로 맞추는 샘플\n",
    "text_only = np.sum(text_hit & ~img_hit & ~mm_hit)\n",
    "img_only = np.sum(~text_hit & img_hit & ~mm_hit)\n",
    "mm_only = np.sum(~text_hit & ~img_hit & mm_hit)\n",
    "\n",
    "# 두 개만 맞추는 경우\n",
    "text_img_only = np.sum(text_hit & img_hit & ~mm_hit)\n",
    "text_mm_only = np.sum(text_hit & ~img_hit & mm_hit)\n",
    "img_mm_only = np.sum(~text_hit & img_hit & mm_hit)\n",
    "\n",
    "# 세 개 모두 맞추는 경우\n",
    "all_three = np.sum(text_hit & img_hit & mm_hit)\n",
    "\n",
    "# 모두 못 맞추는 경우\n",
    "all_miss = np.sum(~text_hit & ~img_hit & ~mm_hit)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Recall@5 Analysis - Embedding Overlap\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n[1] Exclusive Hits - 오직 해당 임베딩만 맞춘 샘플\")\n",
    "print(f\"  Text only:  {text_only:5d} samples ({text_only/len(text_hit)*100:.2f}%)\")\n",
    "print(f\"  Image only: {img_only:5d} samples ({img_only/len(text_hit)*100:.2f}%)\")\n",
    "print(f\"  MM only:    {mm_only:5d} samples ({mm_only/len(text_hit)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n[2] Pairwise Hits - 정확히 두 개만 맞춘 샘플\")\n",
    "print(f\"  Text + Image (MM X): {text_img_only:5d} samples ({text_img_only/len(text_hit)*100:.2f}%)\")\n",
    "print(f\"  Text + MM (Image X): {text_mm_only:5d} samples ({text_mm_only/len(text_hit)*100:.2f}%)\")\n",
    "print(f\"  Image + MM (Text X): {img_mm_only:5d} samples ({img_mm_only/len(text_hit)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n[3] Common & Missing\")\n",
    "print(f\"  All three hit: {all_three:5d} samples ({all_three/len(text_hit)*100:.2f}%)\")\n",
    "print(f\"  All miss:      {all_miss:5d} samples ({all_miss/len(text_hit)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n[4] Verification\")\n",
    "total_check = text_only + img_only + mm_only + text_img_only + text_mm_only + img_mm_only + all_three + all_miss\n",
    "print(f\"  Sum: {text_only} + {img_only} + {mm_only} + {text_img_only} + {text_mm_only} + {img_mm_only} + {all_three} + {all_miss} = {total_check}\")\n",
    "print(f\"  Total samples: {len(text_hit)}\")\n",
    "print(f\"  Match: {total_check == len(text_hit)}\")\n",
    "\n",
    "print(\"\\n[5] Text vs Image (기존 통계)\")\n",
    "print(f\"  Text fail & Image hit: {np.sum(~text_hit & img_hit):5d}\")\n",
    "print(f\"  Text hit & Image fail: {np.sum(text_hit & ~img_hit):5d}\")\n",
    "print(f\"  Both hit:              {np.sum(text_hit & img_hit):5d}\")\n",
    "print(f\"  Both fail:             {np.sum(~text_hit & ~img_hit):5d}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbe00e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_only / total_correct: 0.12347354138398914\n"
     ]
    }
   ],
   "source": [
    "total_correct = 106+182+160+149+13+327+537\n",
    "image_only = 182\n",
    "\n",
    "ratio = image_only / total_correct\n",
    "print(f\"image_only / total_correct: {ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b2645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_correct_qwen2 = 106+182+160+149+13+327+537\n",
    "# image_only_qwen2 = 106\n",
    "\n",
    "# ratio = image_only / total_correct\n",
    "# print(f\"image_only / total_correct: {ratio}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lvlm-rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
